{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Rate Dog: Data Wragling Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data went through the 3 wrangling stages\n",
    "\n",
    "1. Data Gathering\n",
    "\n",
    "2. Accessing the Data\n",
    "\n",
    "3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Gathering\n",
    "\n",
    "\n",
    "The following data gather and used for this project\n",
    "\n",
    "\n",
    "1. Twitter-archive-enhanced.csv was given. It was manually download and the read into the jupyter’s workspace programmatically\n",
    "\n",
    "2. Image_predictions.tsv: This file which contains tweet images and image predictions was downloaded programmatically using the request library\n",
    "\n",
    "3. tweet_json: This was manually downloaded from the Udacity’s resource library as I was not able to open a twitter’s developer account and get a twitter API. The file was programmatically read and necessary data such as tweet_id, favorite_ciunt and retweet_id was extracted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Assessing the Gathered Data\n",
    "\n",
    "1. A visual inspection of the three datasets was first done in order to identify any quality and structural/ tidiness issues in the datasets\n",
    "\n",
    "2. Next, I carried out a Programmatic inspection using  .info(), .describe(), .head(), in order to identify issues with the datatypes, check for missing values, check for outliers amongst others\n",
    "\n",
    "\n",
    "The following Quality and Tidiness issues were identified after programmatically assessing the data\n",
    "\n",
    "\n",
    "#### Quality issues\n",
    "\n",
    "\n",
    "1. Erroneous datatypes in these columns (tweet_id, rating_denominator,rating_numerator, in_reply_to_status_id, in_reply_to_user_id, timestamp, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, doggo, floofer, pupper, and puppo)\n",
    "\n",
    "2. Errors in Dogs name \n",
    "\n",
    "3. 181 records do not contain original tweets from WeRateDogs\n",
    "\n",
    "4. Drop columns not needed for our analysis\n",
    "\n",
    "5. Text column includes a text and a short link\n",
    "\n",
    "6. Source column in the archieve dataset contains HTML-formatted string,this should be categorical\n",
    "\n",
    "7. Some tweets have no image. \n",
    "\n",
    "8. Some values in rating_numerator and rating_denominator seem to be in error or suspicious outliers\n",
    "\n",
    "\n",
    "#### Tidiness issues\n",
    "\n",
    "1. The twitter API table and the image prediction dataset should be merged to twitter_archive dataframe\n",
    "\n",
    "2. The dog stage is being spread across 4 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Data Cleaning\n",
    "\n",
    "\n",
    "\n",
    "A step by step process for cleaning the data was provided – Define, Code and Test. Each of the identified quality and tidiness issues were fixed in the following manner:\n",
    "\n",
    "#### Correcting the Quality Issues\n",
    "\n",
    "1. The erroneous data types were converted to the appropriate data type. \n",
    "\n",
    "2. Typographical errors in dog names were fixed\n",
    "\n",
    "3. Rows not containing original tweets were removed\n",
    "\n",
    "4. Colunms such as ['in_reply_to_status_id', 'in_reply_to_user_id',  'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'] not needed for the analysis were removed\n",
    "\n",
    "5. Hyperlinks in tweets were removed\n",
    "\n",
    "6. I archive datasets which contained HTML formatted string was formatted and extracted from source\n",
    "\n",
    "7. I removed the tweets that had no images\n",
    "\n",
    "8. Rating_numerators which seemed like an error was corrected\n",
    "\n",
    "#### Correcting the Tidiness issues\n",
    "\n",
    "1. The twitter API table and the image prediction dataset were merged with the twitter_archive dataframe\n",
    "\n",
    "2. Merged the dog stage to one column and replaced 'None' with 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Data Storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The cleaned master dataset was saved as CSV file named \"twitter_archive_master.csv\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
